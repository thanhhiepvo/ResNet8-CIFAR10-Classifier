{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMylCC_yvadk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0278fc02-e895-4628-fbae-745a1b7b4ae7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Couldn't find torchinfo... installing it.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    from torchinfo import summary\n",
        "except:\n",
        "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
        "    !pip install -q torchinfo\n",
        "    from torchinfo import summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_B5Rr5xKEDH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "c_tkuJNaNOwK",
        "outputId": "c9bec7a8-4f69-41c1-ba83-7ef0e6b5d13b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2r1iaTKDKPce",
        "outputId": "efca866b-b6e4-472a-d278-b2e615d2bc63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to CIFAR10/train/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:05<00:00, 33.7MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting CIFAR10/train/cifar-10-python.tar.gz to CIFAR10/train\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to CIFAR10/test/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 47.3MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting CIFAR10/test/cifar-10-python.tar.gz to CIFAR10/test\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "train_data = torchvision.datasets.CIFAR10(\n",
        "    root='CIFAR10/train',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    #transform=train_transform,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = torchvision.datasets.CIFAR10(\n",
        "    root='CIFAR10/test',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0gDWpLMKRLz",
        "outputId": "2e3f656b-2dc8-46ad-e0c5-eea4ee790c82"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Dataset CIFAR10\n",
              "     Number of datapoints: 50000\n",
              "     Root location: CIFAR10/train\n",
              "     Split: Train\n",
              "     StandardTransform\n",
              " Transform: ToTensor(),\n",
              " Dataset CIFAR10\n",
              "     Number of datapoints: 10000\n",
              "     Root location: CIFAR10/test\n",
              "     Split: Test\n",
              "     StandardTransform\n",
              " Transform: ToTensor())"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "train_data, test_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oi2KML1ZKU8E",
        "outputId": "a399488a-dc67-4e00-bd10-5350b82369c5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['airplane',\n",
              "  'automobile',\n",
              "  'bird',\n",
              "  'cat',\n",
              "  'deer',\n",
              "  'dog',\n",
              "  'frog',\n",
              "  'horse',\n",
              "  'ship',\n",
              "  'truck'],\n",
              " 10)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "train_data.classes, len(train_data.classes)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Access the data and calculate shape\n",
        "train_data_loader = torch.utils.data.DataLoader(train_data, batch_size=len(train_data))\n",
        "train_data_tensor = next(iter(train_data_loader))[0]\n",
        "print(train_data_tensor.shape)\n",
        "\n",
        "# Calculating mean and std\n",
        "mean = train_data_tensor.mean(dim=[0, 2, 3])\n",
        "std = train_data_tensor.std(dim=[0, 2, 3])\n",
        "\n",
        "print(mean)\n",
        "print(std)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdi6NszyoTrE",
        "outputId": "bbfbe402-df93-476d-82a6-e93c104aeff7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([50000, 3, 32, 32])\n",
            "tensor([0.4914, 0.4822, 0.4465])\n",
            "tensor([0.2470, 0.2435, 0.2616])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(p=0.1),\n",
        "    transforms.RandomVerticalFlip(p=0.1),\n",
        "    transforms.RandomRotation(degrees=(0, 45)),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
        "    transforms.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2470, 0.2435, 0.2616)),\n",
        "    ToTensor()\n",
        "])\n",
        "\n",
        "train_data.train_transform = train_transform"
      ],
      "metadata": {
        "id": "kxxctphnqQG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "EZmzbt4HKals",
        "outputId": "98dfc23c-a727-40bf-ac1b-6d5155226eeb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-0.5, 31.5, 31.5, -0.5)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "image, label = train_data[0]\n",
        "\n",
        "image = image.permute(1, 2, 0)\n",
        "plt.imshow(image.squeeze())\n",
        "plt.title(label)\n",
        "plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQYdmVaJKbXl",
        "outputId": "8ba37cf3-06b2-4e9b-882f-994f68d631b4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<torch.utils.data.dataloader.DataLoader at 0x7cad6ba07070>,\n",
              " <torch.utils.data.dataloader.DataLoader at 0x7cad6f3dea40>)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_data,\n",
        "    batch_size=64,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    dataset=test_data,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "train_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQxb6lHyKeok",
        "outputId": "e719287a-51fe-4081-c657-242639e38e8e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(782, 10000)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "len(train_loader), len(test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MMqE0NHKgK0"
      },
      "outputs": [],
      "source": [
        "examples = enumerate(train_loader)\n",
        "batch_idx, (example_data, example_targets) = next(examples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOpBbsbNKhTD",
        "outputId": "c4d82613-c930-4f6c-9d60-e6f2402be499"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([64, 3, 32, 32]), torch.Size([64]))"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "example_data.shape, example_targets.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "JcCrbBJ45Oz5",
        "outputId": "9d5c87e4-4a77-45b8-b5db-cc7d5aa814fa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nclass CNN(nn.Module):\\n    def __init__(self):\\n        super().__init__() # 3 x 32 x 32\\n        self.block_1 = nn.Sequential(\\n            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=5, padding=2), # 32 x 32 x 32\\n            nn.Dropout(0.35),\\n            nn.BatchNorm2d(32),\\n            nn.ReLU(),\\n            nn.MaxPool2d(kernel_size=2), # 32 x 16 x 16\\n        )\\n        self.block_2 = nn.Sequential(\\n            nn.Conv2d(in_channels=32, out_channels=128, kernel_size=5, padding=2), # 128 x 16 x 16\\n            nn.Dropout(0.35),\\n            nn.BatchNorm2d(128),\\n            nn.ReLU(),\\n            nn.MaxPool2d(kernel_size=2) # 128 x 8 x 8\\n        )\\n        self.block_3 = nn.Sequential(\\n            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5, padding=2), # 128 x 8 x 8\\n            nn.Dropout(0.35),\\n            nn.BatchNorm2d(128),\\n            nn.ReLU(),\\n            nn.MaxPool2d(kernel_size=2), # 128 x 4 x 4\\n        )\\n        self.classifier = nn.Sequential(\\n            nn.Flatten(), # 128 x 4 x 4\\n            nn.Linear(in_features=128*4*4, out_features=256),\\n            nn.Dropout(0.35),\\n            nn.ReLU(),\\n            nn.Linear(in_features=256, out_features=10)\\n        )\\n\\n    def forward(self, x: torch.Tensor):\\n        x = self.block_1(x)\\n        x = self.block_2(x)\\n        x = self.block_3(x)\\n        x = self.classifier(x)\\n        return x\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# AlexNet\n",
        "\"\"\"\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__() # 3 x 32 x 32\n",
        "        self.block_1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=5, padding=2), # 32 x 32 x 32\n",
        "            nn.Dropout(0.35),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2), # 32 x 16 x 16\n",
        "        )\n",
        "        self.block_2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=32, out_channels=128, kernel_size=5, padding=2), # 128 x 16 x 16\n",
        "            nn.Dropout(0.35),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2) # 128 x 8 x 8\n",
        "        )\n",
        "        self.block_3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5, padding=2), # 128 x 8 x 8\n",
        "            nn.Dropout(0.35),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2), # 128 x 4 x 4\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(), # 128 x 4 x 4\n",
        "            nn.Linear(in_features=128*4*4, out_features=256),\n",
        "            nn.Dropout(0.35),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=256, out_features=10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = self.block_1(x)\n",
        "        x = self.block_2(x)\n",
        "        x = self.block_3(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dflon5j7BKJQ"
      },
      "outputs": [],
      "source": [
        "# Resnet Block\n",
        "class ResNet_convert_dim(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride):\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=stride),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myJWIugmBSUx"
      },
      "outputs": [],
      "source": [
        "# Conv + Res block\n",
        "class Conv_Res_block(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, padding=1):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, padding=padding),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, padding=padding),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.res_block = ResNet_convert_dim(in_channels, out_channels, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.in_channels != self.out_channels:\n",
        "            return self.block(x) + self.res_block(x)\n",
        "        return self.block(x) + x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQHdBgJE3APG"
      },
      "outputs": [],
      "source": [
        "# ResNet-10\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__() # 3 x 32 x 32\n",
        "        self.block_1 = Conv_Res_block(in_channels=3, out_channels=32) # 32 x 32 x 32\n",
        "        #self.block_1_2 = Conv_Res_block(in_channels=32, out_channels=32) # 32 x 32 x 32\n",
        "        #self.block_1_3 = Conv_Res_block(in_channels=32, out_channels=32) # 32 x 32 x 32\n",
        "\n",
        "        self.block_2 = Conv_Res_block(in_channels=32, out_channels=64) # 64 x 16 x 16\n",
        "        #self.block_2_2 = Conv_Res_block(in_channels=64, out_channels=64) # 64 x 16 x 16\n",
        "        #self.block_2_3 = Conv_Res_block(in_channels=64, out_channels=64) # 64 x 16 x 16\n",
        "        #self.block_2_4 = Conv_Res_block(in_channels=64, out_channels=64) # 64 x 16 x 16\n",
        "\n",
        "        self.block_3 = Conv_Res_block(in_channels=64, out_channels=128) # 128 x 8 x 8\n",
        "        #self.block_3_2 = Conv_Res_block(in_channels=128, out_channels=128) # 128 x 8 x 8\n",
        "        #self.block_3_3 = Conv_Res_block(in_channels=128, out_channels=128) # 128 x 8 x 8\n",
        "        #self.block_3_4 = Conv_Res_block(in_channels=128, out_channels=128) # 128 x 8 x 8\n",
        "\n",
        "        self.block_4 = Conv_Res_block(in_channels=128, out_channels=256) # 256 x 4 x 4\n",
        "        #self.block_4_2 = Conv_Res_block(in_channels=256, out_channels=256) # 256 x 4 x 4\n",
        "        #self.block_4_3 = Conv_Res_block(in_channels=256, out_channels=256) # 256 x 4 x 4\n",
        "        #self.block_4_4 = Conv_Res_block(in_channels=256, out_channels=256) # 256 x 4 x 4\n",
        "        #self.block_4_5 = Conv_Res_block(in_channels=256, out_channels=256) # 256 x 4 x 4\n",
        "\n",
        "        #self.block_5 = Conv_Res_block(in_channels=256, out_channels=512) # 512 x 2 x 2\n",
        "        #self.block_5_2 = Conv_Res_block(in_channels=512, out_channels=512) # 512 x 2 x 2\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(), # 512 x 2 x 2\n",
        "            nn.Linear(in_features=256*4*4, out_features=1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=1024, out_features=1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            #nn.Dropout(0.5),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=1024, out_features=512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            #nn.Dropout(0.5),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=512, out_features=10)\n",
        "            #nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor): # 3 x 32 x 32\n",
        "        x_1 = self.block_1(x) # 32 x 32 x 32\n",
        "        #x_1 = self.block_1_2(x_1)\n",
        "        #x_1 = self.block_1_3(x_1)\n",
        "        x_1 = nn.MaxPool2d(kernel_size=2)(x_1)\n",
        "\n",
        "        x_2 = self.block_2(x_1) # 64 x 16 x 16\n",
        "        #x_2 = self.block_2_2(x_2)\n",
        "        #x_2 = self.block_2_3(x_2)\n",
        "        #x_2 = self.block_2_4(x_2)\n",
        "        x_2 = nn.MaxPool2d(kernel_size=2)(x_2)\n",
        "\n",
        "        x_3 = self.block_3(x_2) # 128 x 8 x 8\n",
        "        #x_3 = self.block_3_2(x_3)\n",
        "        #x_3 = self.block_3_3(x_3)\n",
        "        #x_3 = self.block_3_4(x_3)\n",
        "        x_3 = nn.MaxPool2d(kernel_size=2)(x_3)\n",
        "\n",
        "        x_4 = self.block_4(x_3) # 256 x 4 x 4\n",
        "        #x_4 = self.block_4_2(x_4)\n",
        "        #x_4 = self.block_4_3(x_4)\n",
        "        #x_4 = self.block_4_4(x_4)\n",
        "        #x_4 = self.block_4_5(x_4)\n",
        "        #x_4 = nn.MaxPool2d(kernel_size=2)(x_4)\n",
        "\n",
        "        #x_5 = self.block_5(x_4) # 512 x 2 x 2\n",
        "        #x_5 = self.block_5_2(x_5)\n",
        "\n",
        "        y = self.classifier(x_4)\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4KnksKpL6AS",
        "outputId": "8df6eb5f-7984-46d1-867e-ca0825df98fa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNN(\n",
              "  (block_1): Conv_Res_block(\n",
              "    (block): Sequential(\n",
              "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): Dropout(p=0.5, inplace=False)\n",
              "      (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (3): ReLU()\n",
              "      (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (5): Dropout(p=0.5, inplace=False)\n",
              "      (6): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (7): ReLU()\n",
              "    )\n",
              "    (res_block): ResNet_convert_dim(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(3, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (block_2): Conv_Res_block(\n",
              "    (block): Sequential(\n",
              "      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): Dropout(p=0.5, inplace=False)\n",
              "      (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (3): ReLU()\n",
              "      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (5): Dropout(p=0.5, inplace=False)\n",
              "      (6): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (7): ReLU()\n",
              "    )\n",
              "    (res_block): ResNet_convert_dim(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (block_3): Conv_Res_block(\n",
              "    (block): Sequential(\n",
              "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): Dropout(p=0.5, inplace=False)\n",
              "      (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (3): ReLU()\n",
              "      (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (5): Dropout(p=0.5, inplace=False)\n",
              "      (6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (7): ReLU()\n",
              "    )\n",
              "    (res_block): ResNet_convert_dim(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (block_4): Conv_Res_block(\n",
              "    (block): Sequential(\n",
              "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): Dropout(p=0.5, inplace=False)\n",
              "      (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (3): ReLU()\n",
              "      (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (5): Dropout(p=0.5, inplace=False)\n",
              "      (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (7): ReLU()\n",
              "    )\n",
              "    (res_block): ResNet_convert_dim(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Flatten(start_dim=1, end_dim=-1)\n",
              "    (1): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "    (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (3): Dropout(p=0.5, inplace=False)\n",
              "    (4): ReLU()\n",
              "    (5): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "    (6): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (7): ReLU()\n",
              "    (8): Linear(in_features=1024, out_features=512, bias=True)\n",
              "    (9): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (10): ReLU()\n",
              "    (11): Linear(in_features=512, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "network = CNN().to(device)\n",
        "network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BkFRLisZvhBj",
        "outputId": "9ab51e4e-80c7-4b15-8b14-2f18d39046e0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "=============================================================================================================================\n",
              "Layer (type (var_name))                       Input Shape          Output Shape         Param #              Trainable\n",
              "=============================================================================================================================\n",
              "CNN (CNN)                                     [1, 3, 32, 32]       [1, 10]              --                   True\n",
              "├─Conv_Res_block (block_1)                    [1, 3, 32, 32]       [1, 32, 32, 32]      --                   True\n",
              "│    └─Sequential (block)                     [1, 3, 32, 32]       [1, 32, 32, 32]      --                   True\n",
              "│    │    └─Conv2d (0)                        [1, 3, 32, 32]       [1, 32, 32, 32]      896                  True\n",
              "│    │    └─Dropout (1)                       [1, 32, 32, 32]      [1, 32, 32, 32]      --                   --\n",
              "│    │    └─BatchNorm2d (2)                   [1, 32, 32, 32]      [1, 32, 32, 32]      64                   True\n",
              "│    │    └─ReLU (3)                          [1, 32, 32, 32]      [1, 32, 32, 32]      --                   --\n",
              "│    │    └─Conv2d (4)                        [1, 32, 32, 32]      [1, 32, 32, 32]      9,248                True\n",
              "│    │    └─Dropout (5)                       [1, 32, 32, 32]      [1, 32, 32, 32]      --                   --\n",
              "│    │    └─BatchNorm2d (6)                   [1, 32, 32, 32]      [1, 32, 32, 32]      64                   True\n",
              "│    │    └─ReLU (7)                          [1, 32, 32, 32]      [1, 32, 32, 32]      --                   --\n",
              "│    └─ResNet_convert_dim (res_block)         [1, 3, 32, 32]       [1, 32, 32, 32]      --                   True\n",
              "│    │    └─Sequential (block)                [1, 3, 32, 32]       [1, 32, 32, 32]      192                  True\n",
              "├─Conv_Res_block (block_2)                    [1, 32, 16, 16]      [1, 64, 16, 16]      --                   True\n",
              "│    └─Sequential (block)                     [1, 32, 16, 16]      [1, 64, 16, 16]      --                   True\n",
              "│    │    └─Conv2d (0)                        [1, 32, 16, 16]      [1, 64, 16, 16]      18,496               True\n",
              "│    │    └─Dropout (1)                       [1, 64, 16, 16]      [1, 64, 16, 16]      --                   --\n",
              "│    │    └─BatchNorm2d (2)                   [1, 64, 16, 16]      [1, 64, 16, 16]      128                  True\n",
              "│    │    └─ReLU (3)                          [1, 64, 16, 16]      [1, 64, 16, 16]      --                   --\n",
              "│    │    └─Conv2d (4)                        [1, 64, 16, 16]      [1, 64, 16, 16]      36,928               True\n",
              "│    │    └─Dropout (5)                       [1, 64, 16, 16]      [1, 64, 16, 16]      --                   --\n",
              "│    │    └─BatchNorm2d (6)                   [1, 64, 16, 16]      [1, 64, 16, 16]      128                  True\n",
              "│    │    └─ReLU (7)                          [1, 64, 16, 16]      [1, 64, 16, 16]      --                   --\n",
              "│    └─ResNet_convert_dim (res_block)         [1, 32, 16, 16]      [1, 64, 16, 16]      --                   True\n",
              "│    │    └─Sequential (block)                [1, 32, 16, 16]      [1, 64, 16, 16]      2,240                True\n",
              "├─Conv_Res_block (block_3)                    [1, 64, 8, 8]        [1, 128, 8, 8]       --                   True\n",
              "│    └─Sequential (block)                     [1, 64, 8, 8]        [1, 128, 8, 8]       --                   True\n",
              "│    │    └─Conv2d (0)                        [1, 64, 8, 8]        [1, 128, 8, 8]       73,856               True\n",
              "│    │    └─Dropout (1)                       [1, 128, 8, 8]       [1, 128, 8, 8]       --                   --\n",
              "│    │    └─BatchNorm2d (2)                   [1, 128, 8, 8]       [1, 128, 8, 8]       256                  True\n",
              "│    │    └─ReLU (3)                          [1, 128, 8, 8]       [1, 128, 8, 8]       --                   --\n",
              "│    │    └─Conv2d (4)                        [1, 128, 8, 8]       [1, 128, 8, 8]       147,584              True\n",
              "│    │    └─Dropout (5)                       [1, 128, 8, 8]       [1, 128, 8, 8]       --                   --\n",
              "│    │    └─BatchNorm2d (6)                   [1, 128, 8, 8]       [1, 128, 8, 8]       256                  True\n",
              "│    │    └─ReLU (7)                          [1, 128, 8, 8]       [1, 128, 8, 8]       --                   --\n",
              "│    └─ResNet_convert_dim (res_block)         [1, 64, 8, 8]        [1, 128, 8, 8]       --                   True\n",
              "│    │    └─Sequential (block)                [1, 64, 8, 8]        [1, 128, 8, 8]       8,576                True\n",
              "├─Conv_Res_block (block_4)                    [1, 128, 4, 4]       [1, 256, 4, 4]       --                   True\n",
              "│    └─Sequential (block)                     [1, 128, 4, 4]       [1, 256, 4, 4]       --                   True\n",
              "│    │    └─Conv2d (0)                        [1, 128, 4, 4]       [1, 256, 4, 4]       295,168              True\n",
              "│    │    └─Dropout (1)                       [1, 256, 4, 4]       [1, 256, 4, 4]       --                   --\n",
              "│    │    └─BatchNorm2d (2)                   [1, 256, 4, 4]       [1, 256, 4, 4]       512                  True\n",
              "│    │    └─ReLU (3)                          [1, 256, 4, 4]       [1, 256, 4, 4]       --                   --\n",
              "│    │    └─Conv2d (4)                        [1, 256, 4, 4]       [1, 256, 4, 4]       590,080              True\n",
              "│    │    └─Dropout (5)                       [1, 256, 4, 4]       [1, 256, 4, 4]       --                   --\n",
              "│    │    └─BatchNorm2d (6)                   [1, 256, 4, 4]       [1, 256, 4, 4]       512                  True\n",
              "│    │    └─ReLU (7)                          [1, 256, 4, 4]       [1, 256, 4, 4]       --                   --\n",
              "│    └─ResNet_convert_dim (res_block)         [1, 128, 4, 4]       [1, 256, 4, 4]       --                   True\n",
              "│    │    └─Sequential (block)                [1, 128, 4, 4]       [1, 256, 4, 4]       33,536               True\n",
              "├─Sequential (classifier)                     [1, 256, 4, 4]       [1, 10]              --                   True\n",
              "│    └─Flatten (0)                            [1, 256, 4, 4]       [1, 4096]            --                   --\n",
              "│    └─Linear (1)                             [1, 4096]            [1, 1024]            4,195,328            True\n",
              "│    └─BatchNorm1d (2)                        [1, 1024]            [1, 1024]            2,048                True\n",
              "│    └─Dropout (3)                            [1, 1024]            [1, 1024]            --                   --\n",
              "│    └─ReLU (4)                               [1, 1024]            [1, 1024]            --                   --\n",
              "│    └─Linear (5)                             [1, 1024]            [1, 1024]            1,049,600            True\n",
              "│    └─BatchNorm1d (6)                        [1, 1024]            [1, 1024]            2,048                True\n",
              "│    └─ReLU (7)                               [1, 1024]            [1, 1024]            --                   --\n",
              "│    └─Linear (8)                             [1, 1024]            [1, 512]             524,800              True\n",
              "│    └─BatchNorm1d (9)                        [1, 512]             [1, 512]             1,024                True\n",
              "│    └─ReLU (10)                              [1, 512]             [1, 512]             --                   --\n",
              "│    └─Linear (11)                            [1, 512]             [1, 10]              5,130                True\n",
              "=============================================================================================================================\n",
              "Total params: 6,998,698\n",
              "Trainable params: 6,998,698\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 60.43\n",
              "=============================================================================================================================\n",
              "Input size (MB): 0.01\n",
              "Forward/backward pass size (MB): 2.99\n",
              "Params size (MB): 27.99\n",
              "Estimated Total Size (MB): 31.00\n",
              "============================================================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "random_input_image = (1, 3, 32, 32)\n",
        "random_input_image_error = (1, 3, 250, 250)\n",
        "\n",
        "summary(CNN(),\n",
        "        input_size=random_input_image,\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BR_EkNfAL-SD",
        "outputId": "4f169700-d4a7-4b4b-eda5-f957c7626a19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading helper_functions.py\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "# Download helper functions from Learn PyTorch repo (if not already downloaded)\n",
        "if Path(\"helper_functions.py\").is_file():\n",
        "  print(\"helper_functions.py already exists, skipping download\")\n",
        "else:\n",
        "  print(\"Downloading helper_functions.py\")\n",
        "  # Note: you need the \"raw\" GitHub URL for this to work\n",
        "  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
        "  with open(\"helper_functions.py\", \"wb\") as f:\n",
        "    f.write(request.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZbpRL18ME4e"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params=network.parameters(),\n",
        "                            lr=0.01,\n",
        "                            momentum=0.9,\n",
        "                            weight_decay=0.0001)\n",
        "\n",
        "from helper_functions import accuracy_fn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0eoa8wxkMGit"
      },
      "outputs": [],
      "source": [
        "def train_step(model: torch.nn.Module,\n",
        "              data_loader: torch.utils.data.DataLoader,\n",
        "              loss_fn: torch.nn.Module,\n",
        "              optimizer: torch.optim.Optimizer,\n",
        "              accuracy_fn,\n",
        "              device):\n",
        "\n",
        "    list_train_loss, list_train_acc = [], []\n",
        "    train_loss, train_acc = 0, 0\n",
        "    for batch, (X, y) in enumerate(data_loader):\n",
        "        # Move X and y to the same device as the model\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # 1. forward pass\n",
        "        y_pred = model(X)\n",
        "\n",
        "        # 2. cal loss\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        train_loss += loss\n",
        "        train_acc += accuracy_fn(y_true=y,\n",
        "                                 y_pred=y_pred.argmax(dim=1))\n",
        "\n",
        "        # 3. optimizer 0 grad\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 4. loss back\n",
        "        loss.backward()\n",
        "\n",
        "        # 5. optimizer step\n",
        "        optimizer.step()\n",
        "\n",
        "        if (batch + 1) % 100 == 0:\n",
        "            train_loss_batch = train_loss / (batch + 1)\n",
        "            train_acc_batch = train_acc / (batch + 1)\n",
        "            print(f\"Train batch: {batch + 1} | Train loss: {train_loss_batch:.5f} | Train acc: {train_acc_batch:.5f}\")\n",
        "\n",
        "    train_loss /= len(data_loader)\n",
        "    train_acc /= len(data_loader)\n",
        "\n",
        "    list_train_loss.append(train_loss)\n",
        "    list_train_acc.append(train_acc)\n",
        "\n",
        "    print(f\"Train loss: {train_loss:.5f} | Train acc: {train_acc:.5f}\")\n",
        "    return list_train_loss, list_train_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNZsqkr4MIdd"
      },
      "outputs": [],
      "source": [
        "def test_step(model: torch.nn.Module,\n",
        "             data_loader: torch.utils.data.DataLoader,\n",
        "             loss_fn: torch.nn.Module,\n",
        "             accuracy_fn,\n",
        "             device):\n",
        "\n",
        "    list_test_loss, list_test_acc = [], []\n",
        "    test_loss, test_acc = 0, 0\n",
        "    model.eval()\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        for batch, (X, y) in enumerate(data_loader):\n",
        "            # Move X and y to the same device as the model\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "            test_pred = model(X)\n",
        "\n",
        "            test_loss += loss_fn(test_pred, y)\n",
        "            test_acc += accuracy_fn(y_true=y,\n",
        "                                    y_pred=test_pred.argmax(dim=1))\n",
        "\n",
        "            if (batch + 1) % 2000 == 0:\n",
        "                test_loss_batch = test_loss / (batch + 1)\n",
        "                test_acc_batch = test_acc / (batch + 1)\n",
        "                print(f\"Test batch: {batch + 1} | Test loss: {test_loss_batch:.5f} | Test acc: {test_acc_batch:.5f}\")\n",
        "\n",
        "        test_loss /= len(data_loader)\n",
        "        test_acc /= len(data_loader)\n",
        "\n",
        "        list_test_loss.append(test_loss)\n",
        "        list_test_acc.append(test_acc)\n",
        "\n",
        "        print(f\"Test loss: {test_loss:.5f} | Test acc: {test_acc:.5f}\\n\")\n",
        "    return list_test_loss, list_test_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "c68df830d8434d1f96c67898d94dd176",
            "b7c3d4c4b8ec4b22a02716889d0d6e23",
            "183f2788fb3f4ae695477a5f74a329bc",
            "51e5ee4bb52d402d98ab494406503a62",
            "7e1f8c74c4b94008bf7f5871a2bcd964",
            "252b5a5fd587425c9a81c4c6a22a6c7e",
            "cd7b9886353c44edb13aab870135d157",
            "316677dd0f234414be35a7be798dd544",
            "24c988b5935541f8b440a3e88991dcd8",
            "2f312247c77f40a3adc6e7458a6af64b",
            "644e11f22b9747afb8f49424f603fd3c"
          ]
        },
        "id": "a1cyGvWIMKAz",
        "collapsed": true,
        "outputId": "1dedaaa0-95af-4d89-f4ba-02cd6efaaf69"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/50 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c68df830d8434d1f96c67898d94dd176"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n",
            "Train batch: 100 | Train loss: 2.01914 | Train acc: 25.42188\n",
            "Train batch: 200 | Train loss: 1.87151 | Train acc: 30.89844\n",
            "Train batch: 300 | Train loss: 1.78889 | Train acc: 33.82292\n",
            "Train batch: 400 | Train loss: 1.72617 | Train acc: 36.21484\n",
            "Train batch: 500 | Train loss: 1.67874 | Train acc: 38.17500\n",
            "Train batch: 600 | Train loss: 1.64319 | Train acc: 39.48438\n",
            "Train batch: 700 | Train loss: 1.61402 | Train acc: 40.69866\n",
            "Train loss: 1.58541 | Train acc: 41.73194\n",
            "Test batch: 2000 | Test loss: 2.74244 | Test acc: 25.20000\n",
            "Test batch: 4000 | Test loss: 2.76240 | Test acc: 25.05000\n",
            "Test batch: 6000 | Test loss: 2.75995 | Test acc: 25.06667\n",
            "Test batch: 8000 | Test loss: 2.75730 | Test acc: 25.08750\n",
            "Test batch: 10000 | Test loss: 2.77970 | Test acc: 24.67000\n",
            "Test loss: 2.77970 | Test acc: 24.67000\n",
            "\n",
            "Epoch: 1\n",
            "Train batch: 100 | Train loss: 1.59570 | Train acc: 42.76562\n",
            "Train batch: 200 | Train loss: 1.48404 | Train acc: 46.32812\n",
            "Train batch: 300 | Train loss: 1.41314 | Train acc: 49.07292\n",
            "Train batch: 400 | Train loss: 1.36527 | Train acc: 50.86719\n",
            "Train batch: 500 | Train loss: 1.33055 | Train acc: 52.06250\n",
            "Train batch: 600 | Train loss: 1.30385 | Train acc: 53.04688\n",
            "Train batch: 700 | Train loss: 1.27841 | Train acc: 53.99554\n",
            "Train loss: 1.25862 | Train acc: 54.75743\n",
            "Test batch: 2000 | Test loss: 1.10165 | Test acc: 59.00000\n",
            "Test batch: 4000 | Test loss: 1.09855 | Test acc: 59.02500\n",
            "Test batch: 6000 | Test loss: 1.09033 | Test acc: 59.35000\n",
            "Test batch: 8000 | Test loss: 1.09832 | Test acc: 59.35000\n",
            "Test batch: 10000 | Test loss: 1.10665 | Test acc: 58.92000\n",
            "Test loss: 1.10665 | Test acc: 58.92000\n",
            "\n",
            "Epoch: 2\n",
            "Train batch: 100 | Train loss: 1.04554 | Train acc: 62.62500\n",
            "Train batch: 200 | Train loss: 1.01531 | Train acc: 63.64062\n",
            "Train batch: 300 | Train loss: 1.00630 | Train acc: 64.29688\n",
            "Train batch: 400 | Train loss: 1.00338 | Train acc: 64.35547\n",
            "Train batch: 500 | Train loss: 1.00017 | Train acc: 64.49375\n",
            "Train batch: 600 | Train loss: 0.99621 | Train acc: 64.67708\n",
            "Train batch: 700 | Train loss: 0.99326 | Train acc: 64.80134\n",
            "Train loss: 0.98390 | Train acc: 65.12948\n",
            "Test batch: 2000 | Test loss: 0.96664 | Test acc: 65.35000\n",
            "Test batch: 4000 | Test loss: 0.97234 | Test acc: 65.40000\n",
            "Test batch: 6000 | Test loss: 0.96700 | Test acc: 65.61667\n",
            "Test batch: 8000 | Test loss: 0.97705 | Test acc: 65.18750\n",
            "Test batch: 10000 | Test loss: 0.98011 | Test acc: 64.99000\n",
            "Test loss: 0.98011 | Test acc: 64.99000\n",
            "\n",
            "Epoch: 3\n",
            "Train batch: 100 | Train loss: 0.82980 | Train acc: 71.10938\n",
            "Train batch: 200 | Train loss: 0.82856 | Train acc: 70.88281\n",
            "Train batch: 300 | Train loss: 0.83773 | Train acc: 70.42188\n",
            "Train batch: 400 | Train loss: 0.83654 | Train acc: 70.40234\n",
            "Train batch: 500 | Train loss: 0.83123 | Train acc: 70.53437\n",
            "Train batch: 600 | Train loss: 0.82850 | Train acc: 70.60417\n",
            "Train batch: 700 | Train loss: 0.82604 | Train acc: 70.69866\n",
            "Train loss: 0.82681 | Train acc: 70.59223\n",
            "Test batch: 2000 | Test loss: 0.87225 | Test acc: 69.40000\n",
            "Test batch: 4000 | Test loss: 0.88208 | Test acc: 68.95000\n",
            "Test batch: 6000 | Test loss: 0.87803 | Test acc: 68.95000\n",
            "Test batch: 8000 | Test loss: 0.88511 | Test acc: 68.57500\n",
            "Test batch: 10000 | Test loss: 0.88724 | Test acc: 68.37000\n",
            "Test loss: 0.88724 | Test acc: 68.37000\n",
            "\n",
            "Epoch: 4\n",
            "Train batch: 100 | Train loss: 0.67620 | Train acc: 76.51562\n",
            "Train batch: 200 | Train loss: 0.66418 | Train acc: 76.92969\n",
            "Train batch: 300 | Train loss: 0.67034 | Train acc: 76.50000\n",
            "Train batch: 400 | Train loss: 0.67543 | Train acc: 76.23047\n",
            "Train batch: 500 | Train loss: 0.67703 | Train acc: 76.16250\n",
            "Train batch: 600 | Train loss: 0.67774 | Train acc: 76.11458\n",
            "Train batch: 700 | Train loss: 0.67558 | Train acc: 76.10045\n",
            "Train loss: 0.67767 | Train acc: 75.98306\n",
            "Test batch: 2000 | Test loss: 0.81906 | Test acc: 71.55000\n",
            "Test batch: 4000 | Test loss: 0.84964 | Test acc: 71.00000\n",
            "Test batch: 6000 | Test loss: 0.84106 | Test acc: 71.28333\n",
            "Test batch: 8000 | Test loss: 0.85479 | Test acc: 70.55000\n",
            "Test batch: 10000 | Test loss: 0.85642 | Test acc: 70.46000\n",
            "Test loss: 0.85642 | Test acc: 70.46000\n",
            "\n",
            "Epoch: 5\n",
            "Train batch: 100 | Train loss: 0.49482 | Train acc: 82.42188\n",
            "Train batch: 200 | Train loss: 0.51649 | Train acc: 81.66406\n",
            "Train batch: 300 | Train loss: 0.52263 | Train acc: 81.55208\n",
            "Train batch: 400 | Train loss: 0.52701 | Train acc: 81.46484\n",
            "Train batch: 500 | Train loss: 0.53420 | Train acc: 81.16875\n",
            "Train batch: 600 | Train loss: 0.53876 | Train acc: 81.03385\n",
            "Train batch: 700 | Train loss: 0.54327 | Train acc: 80.84598\n",
            "Train loss: 0.54510 | Train acc: 80.76646\n",
            "Test batch: 2000 | Test loss: 0.81059 | Test acc: 72.30000\n",
            "Test batch: 4000 | Test loss: 0.82387 | Test acc: 71.62500\n",
            "Test batch: 6000 | Test loss: 0.81177 | Test acc: 72.10000\n",
            "Test batch: 8000 | Test loss: 0.82526 | Test acc: 71.53750\n",
            "Test batch: 10000 | Test loss: 0.81969 | Test acc: 71.82000\n",
            "Test loss: 0.81969 | Test acc: 71.82000\n",
            "\n",
            "Epoch: 6\n",
            "Train batch: 100 | Train loss: 0.36852 | Train acc: 87.34375\n",
            "Train batch: 200 | Train loss: 0.38345 | Train acc: 86.64062\n",
            "Train batch: 300 | Train loss: 0.39554 | Train acc: 86.11979\n",
            "Train batch: 400 | Train loss: 0.40246 | Train acc: 85.85938\n",
            "Train batch: 500 | Train loss: 0.41186 | Train acc: 85.50312\n",
            "Train batch: 600 | Train loss: 0.42173 | Train acc: 85.14062\n",
            "Train batch: 700 | Train loss: 0.42522 | Train acc: 84.96652\n",
            "Train loss: 0.42696 | Train acc: 84.87252\n",
            "Test batch: 2000 | Test loss: 0.89048 | Test acc: 71.65000\n",
            "Test batch: 4000 | Test loss: 0.91371 | Test acc: 71.07500\n",
            "Test batch: 6000 | Test loss: 0.90845 | Test acc: 71.25000\n",
            "Test batch: 8000 | Test loss: 0.91600 | Test acc: 71.10000\n",
            "Test batch: 10000 | Test loss: 0.91322 | Test acc: 71.25000\n",
            "Test loss: 0.91322 | Test acc: 71.25000\n",
            "\n",
            "Epoch: 7\n",
            "Train batch: 100 | Train loss: 0.27847 | Train acc: 90.48438\n",
            "Train batch: 200 | Train loss: 0.27297 | Train acc: 90.45312\n",
            "Train batch: 300 | Train loss: 0.28637 | Train acc: 90.01042\n",
            "Train batch: 400 | Train loss: 0.29633 | Train acc: 89.66406\n",
            "Train batch: 500 | Train loss: 0.30922 | Train acc: 89.10000\n",
            "Train batch: 600 | Train loss: 0.31103 | Train acc: 89.01302\n",
            "Train batch: 700 | Train loss: 0.31924 | Train acc: 88.73884\n",
            "Train loss: 0.32440 | Train acc: 88.55099\n",
            "Test batch: 2000 | Test loss: 0.90232 | Test acc: 73.30000\n",
            "Test batch: 4000 | Test loss: 0.92228 | Test acc: 72.87500\n",
            "Test batch: 6000 | Test loss: 0.93254 | Test acc: 72.65000\n",
            "Test batch: 8000 | Test loss: 0.94436 | Test acc: 72.45000\n",
            "Test batch: 10000 | Test loss: 0.95024 | Test acc: 72.29000\n",
            "Test loss: 0.95024 | Test acc: 72.29000\n",
            "\n",
            "Epoch: 8\n",
            "Train batch: 100 | Train loss: 0.19765 | Train acc: 93.10938\n",
            "Train batch: 200 | Train loss: 0.18998 | Train acc: 93.35938\n",
            "Train batch: 300 | Train loss: 0.20395 | Train acc: 92.85938\n",
            "Train batch: 400 | Train loss: 0.22101 | Train acc: 92.20312\n",
            "Train batch: 500 | Train loss: 0.22708 | Train acc: 91.95625\n",
            "Train batch: 600 | Train loss: 0.23545 | Train acc: 91.59635\n",
            "Train batch: 700 | Train loss: 0.24045 | Train acc: 91.44420\n",
            "Train loss: 0.24333 | Train acc: 91.36029\n",
            "Test batch: 2000 | Test loss: 0.96326 | Test acc: 72.45000\n",
            "Test batch: 4000 | Test loss: 0.99564 | Test acc: 72.17500\n",
            "Test batch: 6000 | Test loss: 0.99043 | Test acc: 72.25000\n",
            "Test batch: 8000 | Test loss: 1.00263 | Test acc: 71.88750\n",
            "Test batch: 10000 | Test loss: 0.99873 | Test acc: 71.95000\n",
            "Test loss: 0.99873 | Test acc: 71.95000\n",
            "\n",
            "Epoch: 9\n",
            "Train batch: 100 | Train loss: 0.15455 | Train acc: 94.31250\n",
            "Train batch: 200 | Train loss: 0.14635 | Train acc: 94.73438\n",
            "Train batch: 300 | Train loss: 0.15741 | Train acc: 94.35938\n",
            "Train batch: 400 | Train loss: 0.16950 | Train acc: 93.94922\n",
            "Train batch: 500 | Train loss: 0.17521 | Train acc: 93.73438\n",
            "Train batch: 600 | Train loss: 0.18182 | Train acc: 93.50260\n",
            "Train batch: 700 | Train loss: 0.18617 | Train acc: 93.36607\n",
            "Train loss: 0.18562 | Train acc: 93.39434\n",
            "Test batch: 2000 | Test loss: 1.06912 | Test acc: 72.95000\n",
            "Test batch: 4000 | Test loss: 1.08101 | Test acc: 73.42500\n",
            "Test batch: 6000 | Test loss: 1.08755 | Test acc: 73.45000\n",
            "Test batch: 8000 | Test loss: 1.11058 | Test acc: 72.93750\n",
            "Test batch: 10000 | Test loss: 1.09880 | Test acc: 72.99000\n",
            "Test loss: 1.09880 | Test acc: 72.99000\n",
            "\n",
            "Epoch: 10\n",
            "Train batch: 100 | Train loss: 0.12169 | Train acc: 95.45312\n",
            "Train batch: 200 | Train loss: 0.12856 | Train acc: 95.34375\n",
            "Train batch: 300 | Train loss: 0.12999 | Train acc: 95.25000\n",
            "Train batch: 400 | Train loss: 0.13871 | Train acc: 95.02344\n",
            "Train batch: 500 | Train loss: 0.14018 | Train acc: 94.96250\n",
            "Train batch: 600 | Train loss: 0.14502 | Train acc: 94.82031\n",
            "Train batch: 700 | Train loss: 0.14778 | Train acc: 94.73438\n",
            "Train loss: 0.14937 | Train acc: 94.68111\n",
            "Test batch: 2000 | Test loss: 1.04826 | Test acc: 73.60000\n",
            "Test batch: 4000 | Test loss: 1.07999 | Test acc: 73.07500\n",
            "Test batch: 6000 | Test loss: 1.06137 | Test acc: 73.51667\n",
            "Test batch: 8000 | Test loss: 1.07638 | Test acc: 73.37500\n",
            "Test batch: 10000 | Test loss: 1.07134 | Test acc: 73.42000\n",
            "Test loss: 1.07134 | Test acc: 73.42000\n",
            "\n",
            "Epoch: 11\n",
            "Train batch: 100 | Train loss: 0.07990 | Train acc: 97.12500\n",
            "Train batch: 200 | Train loss: 0.08326 | Train acc: 97.17188\n",
            "Train batch: 300 | Train loss: 0.09471 | Train acc: 96.83854\n",
            "Train batch: 400 | Train loss: 0.09603 | Train acc: 96.74609\n",
            "Train batch: 500 | Train loss: 0.10366 | Train acc: 96.47500\n",
            "Train batch: 600 | Train loss: 0.10746 | Train acc: 96.29948\n",
            "Train batch: 700 | Train loss: 0.10935 | Train acc: 96.21652\n",
            "Train loss: 0.11300 | Train acc: 96.07976\n",
            "Test batch: 2000 | Test loss: 1.04987 | Test acc: 73.95000\n",
            "Test batch: 4000 | Test loss: 1.06141 | Test acc: 73.42500\n",
            "Test batch: 6000 | Test loss: 1.05281 | Test acc: 74.26667\n",
            "Test batch: 8000 | Test loss: 1.07568 | Test acc: 74.01250\n",
            "Test batch: 10000 | Test loss: 1.08861 | Test acc: 73.88000\n",
            "Test loss: 1.08861 | Test acc: 73.88000\n",
            "\n",
            "Epoch: 12\n",
            "Train batch: 100 | Train loss: 0.08365 | Train acc: 97.00000\n",
            "Train batch: 200 | Train loss: 0.07558 | Train acc: 97.25781\n",
            "Train batch: 300 | Train loss: 0.07692 | Train acc: 97.22396\n",
            "Train batch: 400 | Train loss: 0.07877 | Train acc: 97.16797\n",
            "Train batch: 500 | Train loss: 0.07837 | Train acc: 97.19375\n",
            "Train batch: 600 | Train loss: 0.08425 | Train acc: 96.99479\n",
            "Train batch: 700 | Train loss: 0.09120 | Train acc: 96.71875\n",
            "Train loss: 0.09473 | Train acc: 96.57529\n",
            "Test batch: 2000 | Test loss: 1.16220 | Test acc: 73.60000\n",
            "Test batch: 4000 | Test loss: 1.12970 | Test acc: 74.35000\n",
            "Test batch: 6000 | Test loss: 1.11132 | Test acc: 74.38333\n",
            "Test batch: 8000 | Test loss: 1.14494 | Test acc: 74.01250\n",
            "Test batch: 10000 | Test loss: 1.14821 | Test acc: 73.90000\n",
            "Test loss: 1.14821 | Test acc: 73.90000\n",
            "\n",
            "Epoch: 13\n",
            "Train batch: 100 | Train loss: 0.05726 | Train acc: 98.07812\n",
            "Train batch: 200 | Train loss: 0.06242 | Train acc: 97.96875\n",
            "Train batch: 300 | Train loss: 0.06656 | Train acc: 97.83333\n",
            "Train batch: 400 | Train loss: 0.06947 | Train acc: 97.74219\n",
            "Train batch: 500 | Train loss: 0.07423 | Train acc: 97.51875\n",
            "Train batch: 600 | Train loss: 0.07477 | Train acc: 97.47917\n",
            "Train batch: 700 | Train loss: 0.07609 | Train acc: 97.41071\n",
            "Train loss: 0.07871 | Train acc: 97.31258\n",
            "Test batch: 2000 | Test loss: 1.27045 | Test acc: 74.20000\n",
            "Test batch: 4000 | Test loss: 1.24012 | Test acc: 74.12500\n",
            "Test batch: 6000 | Test loss: 1.21706 | Test acc: 74.25000\n",
            "Test batch: 8000 | Test loss: 1.25146 | Test acc: 73.77500\n",
            "Test batch: 10000 | Test loss: 1.25884 | Test acc: 73.96000\n",
            "Test loss: 1.25884 | Test acc: 73.96000\n",
            "\n",
            "Epoch: 14\n",
            "Train batch: 100 | Train loss: 0.04525 | Train acc: 98.39062\n",
            "Train batch: 200 | Train loss: 0.04114 | Train acc: 98.57812\n",
            "Train batch: 300 | Train loss: 0.04620 | Train acc: 98.42708\n",
            "Train batch: 400 | Train loss: 0.04872 | Train acc: 98.31250\n",
            "Train batch: 500 | Train loss: 0.05423 | Train acc: 98.10938\n",
            "Train batch: 600 | Train loss: 0.05676 | Train acc: 98.02865\n",
            "Train batch: 700 | Train loss: 0.05684 | Train acc: 98.03125\n",
            "Train loss: 0.06158 | Train acc: 97.83808\n",
            "Test batch: 2000 | Test loss: 1.32597 | Test acc: 73.30000\n",
            "Test batch: 4000 | Test loss: 1.29298 | Test acc: 73.47500\n",
            "Test batch: 6000 | Test loss: 1.28259 | Test acc: 73.78333\n",
            "Test batch: 8000 | Test loss: 1.30294 | Test acc: 73.45000\n",
            "Test batch: 10000 | Test loss: 1.30196 | Test acc: 73.51000\n",
            "Test loss: 1.30196 | Test acc: 73.51000\n",
            "\n",
            "Epoch: 15\n",
            "Train batch: 100 | Train loss: 0.07177 | Train acc: 97.57812\n",
            "Train batch: 200 | Train loss: 0.05929 | Train acc: 97.95312\n",
            "Train batch: 300 | Train loss: 0.05819 | Train acc: 97.97396\n",
            "Train batch: 400 | Train loss: 0.05825 | Train acc: 97.97266\n",
            "Train batch: 500 | Train loss: 0.05789 | Train acc: 97.98125\n",
            "Train batch: 600 | Train loss: 0.05913 | Train acc: 97.94531\n",
            "Train batch: 700 | Train loss: 0.05979 | Train acc: 97.91518\n",
            "Train loss: 0.05781 | Train acc: 97.97594\n",
            "Test batch: 2000 | Test loss: 1.29253 | Test acc: 75.40000\n",
            "Test batch: 4000 | Test loss: 1.28752 | Test acc: 75.17500\n",
            "Test batch: 6000 | Test loss: 1.30179 | Test acc: 75.36667\n",
            "Test batch: 8000 | Test loss: 1.32807 | Test acc: 75.15000\n",
            "Test batch: 10000 | Test loss: 1.31415 | Test acc: 75.29000\n",
            "Test loss: 1.31415 | Test acc: 75.29000\n",
            "\n",
            "Epoch: 16\n",
            "Train batch: 100 | Train loss: 0.03529 | Train acc: 98.85938\n",
            "Train batch: 200 | Train loss: 0.03969 | Train acc: 98.65625\n",
            "Train batch: 300 | Train loss: 0.04390 | Train acc: 98.46875\n",
            "Train batch: 400 | Train loss: 0.04382 | Train acc: 98.46094\n",
            "Train batch: 500 | Train loss: 0.04292 | Train acc: 98.48750\n",
            "Train batch: 600 | Train loss: 0.04339 | Train acc: 98.48177\n",
            "Train batch: 700 | Train loss: 0.04800 | Train acc: 98.33259\n",
            "Train loss: 0.04859 | Train acc: 98.30962\n",
            "Test batch: 2000 | Test loss: 1.25162 | Test acc: 75.05000\n",
            "Test batch: 4000 | Test loss: 1.30052 | Test acc: 74.70000\n",
            "Test batch: 6000 | Test loss: 1.29732 | Test acc: 74.93333\n",
            "Test batch: 8000 | Test loss: 1.32134 | Test acc: 74.55000\n",
            "Test batch: 10000 | Test loss: 1.32656 | Test acc: 74.61000\n",
            "Test loss: 1.32656 | Test acc: 74.61000\n",
            "\n",
            "Epoch: 17\n",
            "Train batch: 100 | Train loss: 0.05298 | Train acc: 98.23438\n",
            "Train batch: 200 | Train loss: 0.04848 | Train acc: 98.31250\n",
            "Train batch: 300 | Train loss: 0.04454 | Train acc: 98.44792\n",
            "Train batch: 400 | Train loss: 0.04197 | Train acc: 98.53125\n",
            "Train batch: 500 | Train loss: 0.04068 | Train acc: 98.61562\n",
            "Train batch: 600 | Train loss: 0.04058 | Train acc: 98.62760\n",
            "Train batch: 700 | Train loss: 0.04163 | Train acc: 98.59598\n",
            "Train loss: 0.04174 | Train acc: 98.59135\n",
            "Test batch: 2000 | Test loss: 1.21368 | Test acc: 77.75000\n",
            "Test batch: 4000 | Test loss: 1.25401 | Test acc: 76.75000\n",
            "Test batch: 6000 | Test loss: 1.25691 | Test acc: 76.61667\n",
            "Test batch: 8000 | Test loss: 1.28058 | Test acc: 76.16250\n",
            "Test batch: 10000 | Test loss: 1.28157 | Test acc: 76.16000\n",
            "Test loss: 1.28157 | Test acc: 76.16000\n",
            "\n",
            "Epoch: 18\n",
            "Train batch: 100 | Train loss: 0.03198 | Train acc: 98.89062\n",
            "Train batch: 200 | Train loss: 0.02629 | Train acc: 99.14062\n",
            "Train batch: 300 | Train loss: 0.02405 | Train acc: 99.23438\n",
            "Train batch: 400 | Train loss: 0.02665 | Train acc: 99.14453\n",
            "Train batch: 500 | Train loss: 0.02702 | Train acc: 99.13125\n",
            "Train batch: 600 | Train loss: 0.03002 | Train acc: 99.04167\n",
            "Train batch: 700 | Train loss: 0.03211 | Train acc: 98.95759\n",
            "Train loss: 0.03324 | Train acc: 98.91904\n",
            "Test batch: 2000 | Test loss: 1.28347 | Test acc: 74.50000\n",
            "Test batch: 4000 | Test loss: 1.34295 | Test acc: 74.12500\n",
            "Test batch: 6000 | Test loss: 1.33610 | Test acc: 74.35000\n",
            "Test batch: 8000 | Test loss: 1.35962 | Test acc: 74.13750\n",
            "Test batch: 10000 | Test loss: 1.35786 | Test acc: 74.10000\n",
            "Test loss: 1.35786 | Test acc: 74.10000\n",
            "\n",
            "Epoch: 19\n",
            "Train batch: 100 | Train loss: 0.02355 | Train acc: 99.09375\n",
            "Train batch: 200 | Train loss: 0.02619 | Train acc: 99.09375\n",
            "Train batch: 300 | Train loss: 0.02524 | Train acc: 99.13021\n",
            "Train batch: 400 | Train loss: 0.02300 | Train acc: 99.19531\n",
            "Train batch: 500 | Train loss: 0.02246 | Train acc: 99.20312\n",
            "Train batch: 600 | Train loss: 0.02665 | Train acc: 99.04427\n",
            "Train batch: 700 | Train loss: 0.02647 | Train acc: 99.06250\n",
            "Train loss: 0.02633 | Train acc: 99.06090\n",
            "Test batch: 2000 | Test loss: 1.39140 | Test acc: 75.40000\n",
            "Test batch: 4000 | Test loss: 1.41857 | Test acc: 74.70000\n",
            "Test batch: 6000 | Test loss: 1.40598 | Test acc: 74.90000\n",
            "Test batch: 8000 | Test loss: 1.42999 | Test acc: 74.65000\n",
            "Test batch: 10000 | Test loss: 1.42988 | Test acc: 74.65000\n",
            "Test loss: 1.42988 | Test acc: 74.65000\n",
            "\n",
            "Epoch: 20\n",
            "Train batch: 100 | Train loss: 0.02222 | Train acc: 99.29688\n",
            "Train batch: 200 | Train loss: 0.02015 | Train acc: 99.37500\n",
            "Train batch: 300 | Train loss: 0.01690 | Train acc: 99.46354\n",
            "Train batch: 400 | Train loss: 0.01663 | Train acc: 99.47656\n",
            "Train batch: 500 | Train loss: 0.01666 | Train acc: 99.47187\n",
            "Train batch: 600 | Train loss: 0.01759 | Train acc: 99.44271\n",
            "Train batch: 700 | Train loss: 0.02096 | Train acc: 99.32143\n",
            "Train loss: 0.02095 | Train acc: 99.32465\n",
            "Test batch: 2000 | Test loss: 1.46149 | Test acc: 74.95000\n",
            "Test batch: 4000 | Test loss: 1.47041 | Test acc: 74.42500\n",
            "Test batch: 6000 | Test loss: 1.45571 | Test acc: 74.61667\n",
            "Test batch: 8000 | Test loss: 1.46247 | Test acc: 74.55000\n",
            "Test batch: 10000 | Test loss: 1.46185 | Test acc: 74.46000\n",
            "Test loss: 1.46185 | Test acc: 74.46000\n",
            "\n",
            "Epoch: 21\n",
            "Train batch: 100 | Train loss: 0.02837 | Train acc: 98.96875\n",
            "Train batch: 200 | Train loss: 0.02221 | Train acc: 99.21094\n",
            "Train batch: 300 | Train loss: 0.02142 | Train acc: 99.23958\n",
            "Train batch: 400 | Train loss: 0.02681 | Train acc: 99.07422\n",
            "Train batch: 500 | Train loss: 0.02814 | Train acc: 99.02813\n",
            "Train batch: 600 | Train loss: 0.02746 | Train acc: 99.05208\n",
            "Train batch: 700 | Train loss: 0.02979 | Train acc: 98.97321\n",
            "Train loss: 0.02960 | Train acc: 98.97299\n",
            "Test batch: 2000 | Test loss: 1.39867 | Test acc: 74.25000\n",
            "Test batch: 4000 | Test loss: 1.39703 | Test acc: 74.65000\n",
            "Test batch: 6000 | Test loss: 1.38226 | Test acc: 75.06667\n",
            "Test batch: 8000 | Test loss: 1.39725 | Test acc: 74.91250\n",
            "Test batch: 10000 | Test loss: 1.39379 | Test acc: 74.96000\n",
            "Test loss: 1.39379 | Test acc: 74.96000\n",
            "\n",
            "Epoch: 22\n",
            "Train batch: 100 | Train loss: 0.01998 | Train acc: 99.32812\n",
            "Train batch: 200 | Train loss: 0.01831 | Train acc: 99.40625\n",
            "Train batch: 300 | Train loss: 0.01653 | Train acc: 99.45833\n",
            "Train batch: 400 | Train loss: 0.01520 | Train acc: 99.50391\n",
            "Train batch: 500 | Train loss: 0.01352 | Train acc: 99.57188\n",
            "Train batch: 600 | Train loss: 0.01259 | Train acc: 99.60417\n",
            "Train batch: 700 | Train loss: 0.01267 | Train acc: 99.59821\n",
            "Train loss: 0.01393 | Train acc: 99.55842\n",
            "Test batch: 2000 | Test loss: 1.37378 | Test acc: 75.50000\n",
            "Test batch: 4000 | Test loss: 1.39606 | Test acc: 75.87500\n",
            "Test batch: 6000 | Test loss: 1.42065 | Test acc: 75.55000\n",
            "Test batch: 8000 | Test loss: 1.46175 | Test acc: 75.10000\n",
            "Test batch: 10000 | Test loss: 1.46438 | Test acc: 75.12000\n",
            "Test loss: 1.46438 | Test acc: 75.12000\n",
            "\n",
            "Epoch: 23\n",
            "Train batch: 100 | Train loss: 0.01339 | Train acc: 99.60938\n",
            "Train batch: 200 | Train loss: 0.01315 | Train acc: 99.64062\n",
            "Train batch: 300 | Train loss: 0.01153 | Train acc: 99.67188\n",
            "Train batch: 400 | Train loss: 0.01069 | Train acc: 99.66406\n",
            "Train batch: 500 | Train loss: 0.01007 | Train acc: 99.69688\n",
            "Train batch: 600 | Train loss: 0.00921 | Train acc: 99.72396\n",
            "Train batch: 700 | Train loss: 0.00878 | Train acc: 99.74107\n",
            "Train loss: 0.00901 | Train acc: 99.73026\n",
            "Test batch: 2000 | Test loss: 1.40232 | Test acc: 76.40000\n",
            "Test batch: 4000 | Test loss: 1.43525 | Test acc: 76.15000\n",
            "Test batch: 6000 | Test loss: 1.43841 | Test acc: 76.15000\n",
            "Test batch: 8000 | Test loss: 1.46947 | Test acc: 75.83750\n",
            "Test batch: 10000 | Test loss: 1.47510 | Test acc: 75.96000\n",
            "Test loss: 1.47510 | Test acc: 75.96000\n",
            "\n",
            "Epoch: 24\n",
            "Train batch: 100 | Train loss: 0.00561 | Train acc: 99.78125\n",
            "Train batch: 200 | Train loss: 0.00848 | Train acc: 99.71875\n",
            "Train batch: 300 | Train loss: 0.00880 | Train acc: 99.71875\n",
            "Train batch: 400 | Train loss: 0.00754 | Train acc: 99.76172\n",
            "Train batch: 500 | Train loss: 0.00705 | Train acc: 99.77500\n",
            "Train batch: 600 | Train loss: 0.00645 | Train acc: 99.80208\n",
            "Train batch: 700 | Train loss: 0.00583 | Train acc: 99.82589\n",
            "Train loss: 0.00550 | Train acc: 99.83416\n",
            "Test batch: 2000 | Test loss: 1.33723 | Test acc: 76.55000\n",
            "Test batch: 4000 | Test loss: 1.37808 | Test acc: 76.67500\n",
            "Test batch: 6000 | Test loss: 1.40055 | Test acc: 76.53333\n",
            "Test batch: 8000 | Test loss: 1.44715 | Test acc: 76.35000\n",
            "Test batch: 10000 | Test loss: 1.44829 | Test acc: 76.16000\n",
            "Test loss: 1.44829 | Test acc: 76.16000\n",
            "\n",
            "Epoch: 25\n",
            "Train batch: 100 | Train loss: 0.00397 | Train acc: 99.92188\n",
            "Train batch: 200 | Train loss: 0.00313 | Train acc: 99.92188\n",
            "Train batch: 300 | Train loss: 0.00241 | Train acc: 99.93750\n",
            "Train batch: 400 | Train loss: 0.00214 | Train acc: 99.94141\n",
            "Train batch: 500 | Train loss: 0.00213 | Train acc: 99.94375\n",
            "Train batch: 600 | Train loss: 0.00239 | Train acc: 99.93750\n",
            "Train batch: 700 | Train loss: 0.00305 | Train acc: 99.91741\n",
            "Train loss: 0.00374 | Train acc: 99.89011\n",
            "Test batch: 2000 | Test loss: 1.44082 | Test acc: 75.90000\n",
            "Test batch: 4000 | Test loss: 1.44359 | Test acc: 75.90000\n",
            "Test batch: 6000 | Test loss: 1.44231 | Test acc: 75.90000\n",
            "Test batch: 8000 | Test loss: 1.48369 | Test acc: 75.85000\n",
            "Test batch: 10000 | Test loss: 1.49048 | Test acc: 75.74000\n",
            "Test loss: 1.49048 | Test acc: 75.74000\n",
            "\n",
            "Epoch: 26\n",
            "Train batch: 100 | Train loss: 0.00359 | Train acc: 99.87500\n",
            "Train batch: 200 | Train loss: 0.00288 | Train acc: 99.91406\n",
            "Train batch: 300 | Train loss: 0.00348 | Train acc: 99.89062\n",
            "Train batch: 400 | Train loss: 0.00304 | Train acc: 99.91016\n",
            "Train batch: 500 | Train loss: 0.00309 | Train acc: 99.91563\n",
            "Train batch: 600 | Train loss: 0.00347 | Train acc: 99.90365\n",
            "Train batch: 700 | Train loss: 0.00352 | Train acc: 99.88839\n",
            "Train loss: 0.00363 | Train acc: 99.88611\n",
            "Test batch: 2000 | Test loss: 1.37000 | Test acc: 76.80000\n",
            "Test batch: 4000 | Test loss: 1.39004 | Test acc: 76.60000\n",
            "Test batch: 6000 | Test loss: 1.39247 | Test acc: 76.56667\n",
            "Test batch: 8000 | Test loss: 1.41664 | Test acc: 76.36250\n",
            "Test batch: 10000 | Test loss: 1.41674 | Test acc: 76.30000\n",
            "Test loss: 1.41674 | Test acc: 76.30000\n",
            "\n",
            "Epoch: 27\n",
            "Train batch: 100 | Train loss: 0.00068 | Train acc: 100.00000\n",
            "Train batch: 200 | Train loss: 0.00060 | Train acc: 100.00000\n",
            "Train batch: 300 | Train loss: 0.00052 | Train acc: 100.00000\n",
            "Train batch: 400 | Train loss: 0.00050 | Train acc: 100.00000\n",
            "Train batch: 500 | Train loss: 0.00055 | Train acc: 99.99688\n",
            "Train batch: 600 | Train loss: 0.00056 | Train acc: 99.99740\n",
            "Train batch: 700 | Train loss: 0.00072 | Train acc: 99.99107\n",
            "Train loss: 0.00073 | Train acc: 99.99001\n",
            "Test batch: 2000 | Test loss: 1.33218 | Test acc: 77.80000\n",
            "Test batch: 4000 | Test loss: 1.34848 | Test acc: 77.37500\n",
            "Test batch: 6000 | Test loss: 1.35240 | Test acc: 77.03333\n",
            "Test batch: 8000 | Test loss: 1.38244 | Test acc: 76.82500\n",
            "Test batch: 10000 | Test loss: 1.37955 | Test acc: 76.77000\n",
            "Test loss: 1.37955 | Test acc: 76.77000\n",
            "\n",
            "Epoch: 28\n",
            "Train batch: 100 | Train loss: 0.00022 | Train acc: 100.00000\n",
            "Train batch: 200 | Train loss: 0.00028 | Train acc: 100.00000\n",
            "Train batch: 300 | Train loss: 0.00027 | Train acc: 100.00000\n",
            "Train batch: 400 | Train loss: 0.00025 | Train acc: 100.00000\n",
            "Train batch: 500 | Train loss: 0.00023 | Train acc: 100.00000\n",
            "Train batch: 600 | Train loss: 0.00023 | Train acc: 100.00000\n",
            "Train batch: 700 | Train loss: 0.00022 | Train acc: 100.00000\n",
            "Train loss: 0.00022 | Train acc: 100.00000\n",
            "Test batch: 2000 | Test loss: 1.27463 | Test acc: 77.10000\n",
            "Test batch: 4000 | Test loss: 1.29185 | Test acc: 77.07500\n",
            "Test batch: 6000 | Test loss: 1.29571 | Test acc: 77.20000\n",
            "Test batch: 8000 | Test loss: 1.32085 | Test acc: 77.01250\n",
            "Test batch: 10000 | Test loss: 1.31820 | Test acc: 77.08000\n",
            "Test loss: 1.31820 | Test acc: 77.08000\n",
            "\n",
            "Epoch: 29\n",
            "Train batch: 100 | Train loss: 0.00015 | Train acc: 100.00000\n",
            "Train batch: 200 | Train loss: 0.00015 | Train acc: 100.00000\n",
            "Train batch: 300 | Train loss: 0.00015 | Train acc: 100.00000\n",
            "Train batch: 400 | Train loss: 0.00015 | Train acc: 100.00000\n",
            "Train batch: 500 | Train loss: 0.00016 | Train acc: 100.00000\n",
            "Train batch: 600 | Train loss: 0.00016 | Train acc: 100.00000\n",
            "Train batch: 700 | Train loss: 0.00016 | Train acc: 100.00000\n",
            "Train loss: 0.00016 | Train acc: 100.00000\n",
            "Test batch: 2000 | Test loss: 1.23210 | Test acc: 77.35000\n",
            "Test batch: 4000 | Test loss: 1.24755 | Test acc: 77.27500\n",
            "Test batch: 6000 | Test loss: 1.24894 | Test acc: 77.38333\n",
            "Test batch: 8000 | Test loss: 1.27310 | Test acc: 77.16250\n",
            "Test batch: 10000 | Test loss: 1.26971 | Test acc: 77.24000\n",
            "Test loss: 1.26971 | Test acc: 77.24000\n",
            "\n",
            "Epoch: 30\n",
            "Train batch: 100 | Train loss: 0.00016 | Train acc: 100.00000\n",
            "Train batch: 200 | Train loss: 0.00017 | Train acc: 100.00000\n",
            "Train batch: 300 | Train loss: 0.00017 | Train acc: 100.00000\n",
            "Train batch: 400 | Train loss: 0.00018 | Train acc: 100.00000\n",
            "Train batch: 500 | Train loss: 0.00018 | Train acc: 100.00000\n",
            "Train batch: 600 | Train loss: 0.00018 | Train acc: 100.00000\n",
            "Train batch: 700 | Train loss: 0.00019 | Train acc: 100.00000\n",
            "Train loss: 0.00019 | Train acc: 100.00000\n",
            "Test batch: 2000 | Test loss: 1.19908 | Test acc: 77.55000\n",
            "Test batch: 4000 | Test loss: 1.21317 | Test acc: 77.35000\n",
            "Test batch: 6000 | Test loss: 1.21452 | Test acc: 77.36667\n",
            "Test batch: 8000 | Test loss: 1.23784 | Test acc: 77.13750\n",
            "Test batch: 10000 | Test loss: 1.23354 | Test acc: 77.25000\n",
            "Test loss: 1.23354 | Test acc: 77.25000\n",
            "\n",
            "Epoch: 31\n",
            "Train batch: 100 | Train loss: 0.00019 | Train acc: 100.00000\n",
            "Train batch: 200 | Train loss: 0.00019 | Train acc: 100.00000\n",
            "Train batch: 300 | Train loss: 0.00020 | Train acc: 100.00000\n",
            "Train batch: 400 | Train loss: 0.00020 | Train acc: 100.00000\n",
            "Train batch: 500 | Train loss: 0.00021 | Train acc: 100.00000\n",
            "Train batch: 600 | Train loss: 0.00021 | Train acc: 100.00000\n",
            "Train batch: 700 | Train loss: 0.00022 | Train acc: 100.00000\n",
            "Train loss: 0.00022 | Train acc: 100.00000\n",
            "Test batch: 2000 | Test loss: 1.17359 | Test acc: 77.55000\n",
            "Test batch: 4000 | Test loss: 1.18602 | Test acc: 77.40000\n",
            "Test batch: 6000 | Test loss: 1.18730 | Test acc: 77.43333\n",
            "Test batch: 8000 | Test loss: 1.20949 | Test acc: 77.17500\n",
            "Test batch: 10000 | Test loss: 1.20425 | Test acc: 77.33000\n",
            "Test loss: 1.20425 | Test acc: 77.33000\n",
            "\n",
            "Epoch: 32\n",
            "Train batch: 100 | Train loss: 0.00022 | Train acc: 100.00000\n",
            "Train batch: 200 | Train loss: 0.00022 | Train acc: 100.00000\n",
            "Train batch: 300 | Train loss: 0.00023 | Train acc: 100.00000\n",
            "Train batch: 400 | Train loss: 0.00023 | Train acc: 100.00000\n",
            "Train batch: 500 | Train loss: 0.00024 | Train acc: 100.00000\n",
            "Train batch: 600 | Train loss: 0.00024 | Train acc: 100.00000\n",
            "Train batch: 700 | Train loss: 0.00025 | Train acc: 100.00000\n",
            "Train loss: 0.00025 | Train acc: 100.00000\n",
            "Test batch: 2000 | Test loss: 1.15293 | Test acc: 77.60000\n",
            "Test batch: 4000 | Test loss: 1.16456 | Test acc: 77.47500\n",
            "Test batch: 6000 | Test loss: 1.16596 | Test acc: 77.55000\n",
            "Test batch: 8000 | Test loss: 1.18729 | Test acc: 77.31250\n",
            "Test batch: 10000 | Test loss: 1.18154 | Test acc: 77.42000\n",
            "Test loss: 1.18154 | Test acc: 77.42000\n",
            "\n",
            "Epoch: 33\n",
            "Train batch: 100 | Train loss: 0.00025 | Train acc: 100.00000\n",
            "Train batch: 200 | Train loss: 0.00026 | Train acc: 100.00000\n",
            "Train batch: 300 | Train loss: 0.00026 | Train acc: 100.00000\n",
            "Train batch: 400 | Train loss: 0.00027 | Train acc: 100.00000\n",
            "Train batch: 500 | Train loss: 0.00027 | Train acc: 100.00000\n",
            "Train batch: 600 | Train loss: 0.00028 | Train acc: 100.00000\n",
            "Train batch: 700 | Train loss: 0.00028 | Train acc: 100.00000\n",
            "Train loss: 0.00028 | Train acc: 100.00000\n",
            "Test batch: 2000 | Test loss: 1.13834 | Test acc: 77.50000\n",
            "Test batch: 4000 | Test loss: 1.14914 | Test acc: 77.50000\n",
            "Test batch: 6000 | Test loss: 1.15033 | Test acc: 77.51667\n",
            "Test batch: 8000 | Test loss: 1.17093 | Test acc: 77.23750\n",
            "Test batch: 10000 | Test loss: 1.16451 | Test acc: 77.42000\n",
            "Test loss: 1.16451 | Test acc: 77.42000\n",
            "\n",
            "Epoch: 34\n",
            "Train batch: 100 | Train loss: 0.00028 | Train acc: 100.00000\n",
            "Train batch: 200 | Train loss: 0.00029 | Train acc: 100.00000\n",
            "Train batch: 300 | Train loss: 0.00028 | Train acc: 100.00000\n",
            "Train batch: 400 | Train loss: 0.00029 | Train acc: 100.00000\n",
            "Train batch: 500 | Train loss: 0.00029 | Train acc: 100.00000\n",
            "Train batch: 600 | Train loss: 0.00030 | Train acc: 100.00000\n",
            "Train batch: 700 | Train loss: 0.00030 | Train acc: 100.00000\n",
            "Train loss: 0.00031 | Train acc: 100.00000\n",
            "Test batch: 2000 | Test loss: 1.12735 | Test acc: 77.80000\n",
            "Test batch: 4000 | Test loss: 1.13752 | Test acc: 77.65000\n",
            "Test batch: 6000 | Test loss: 1.13893 | Test acc: 77.51667\n",
            "Test batch: 8000 | Test loss: 1.15853 | Test acc: 77.30000\n",
            "Test batch: 10000 | Test loss: 1.15117 | Test acc: 77.44000\n",
            "Test loss: 1.15117 | Test acc: 77.44000\n",
            "\n",
            "Epoch: 35\n",
            "Train batch: 100 | Train loss: 0.00032 | Train acc: 100.00000\n",
            "Train batch: 200 | Train loss: 0.00030 | Train acc: 100.00000\n",
            "Train batch: 300 | Train loss: 0.00030 | Train acc: 100.00000\n",
            "Train batch: 400 | Train loss: 0.00031 | Train acc: 100.00000\n",
            "Train batch: 500 | Train loss: 0.00032 | Train acc: 100.00000\n",
            "Train batch: 600 | Train loss: 0.00032 | Train acc: 100.00000\n",
            "Train batch: 700 | Train loss: 0.00033 | Train acc: 100.00000\n",
            "Train loss: 0.00033 | Train acc: 100.00000\n",
            "Test batch: 2000 | Test loss: 1.11862 | Test acc: 77.95000\n",
            "Test batch: 4000 | Test loss: 1.12804 | Test acc: 77.62500\n",
            "Test batch: 6000 | Test loss: 1.12923 | Test acc: 77.53333\n",
            "Test batch: 8000 | Test loss: 1.14824 | Test acc: 77.31250\n",
            "Test batch: 10000 | Test loss: 1.14017 | Test acc: 77.44000\n",
            "Test loss: 1.14017 | Test acc: 77.44000\n",
            "\n",
            "Epoch: 36\n",
            "Train batch: 100 | Train loss: 0.00031 | Train acc: 100.00000\n",
            "Train batch: 200 | Train loss: 0.00032 | Train acc: 100.00000\n",
            "Train batch: 300 | Train loss: 0.00033 | Train acc: 100.00000\n",
            "Train batch: 400 | Train loss: 0.00034 | Train acc: 100.00000\n",
            "Train batch: 500 | Train loss: 0.00034 | Train acc: 100.00000\n",
            "Train batch: 600 | Train loss: 0.00034 | Train acc: 100.00000\n",
            "Train batch: 700 | Train loss: 0.00035 | Train acc: 100.00000\n",
            "Train loss: 0.00035 | Train acc: 100.00000\n",
            "Test batch: 2000 | Test loss: 1.11092 | Test acc: 77.75000\n",
            "Test batch: 4000 | Test loss: 1.12005 | Test acc: 77.47500\n",
            "Test batch: 6000 | Test loss: 1.12096 | Test acc: 77.46667\n",
            "Test batch: 8000 | Test loss: 1.13944 | Test acc: 77.26250\n",
            "Test batch: 10000 | Test loss: 1.13010 | Test acc: 77.49000\n",
            "Test loss: 1.13010 | Test acc: 77.49000\n",
            "\n",
            "Epoch: 37\n",
            "Train batch: 100 | Train loss: 0.00036 | Train acc: 100.00000\n",
            "Train batch: 200 | Train loss: 0.00034 | Train acc: 100.00000\n",
            "Train batch: 300 | Train loss: 0.00034 | Train acc: 100.00000\n",
            "Train batch: 400 | Train loss: 0.00035 | Train acc: 100.00000\n",
            "Train batch: 500 | Train loss: 0.00036 | Train acc: 100.00000\n",
            "Train batch: 600 | Train loss: 0.00036 | Train acc: 100.00000\n",
            "Train batch: 700 | Train loss: 0.00037 | Train acc: 100.00000\n",
            "Train loss: 0.00037 | Train acc: 100.00000\n",
            "Test batch: 2000 | Test loss: 1.10803 | Test acc: 77.95000\n",
            "Test batch: 4000 | Test loss: 1.11495 | Test acc: 77.52500\n",
            "Test batch: 6000 | Test loss: 1.11640 | Test acc: 77.41667\n",
            "Test batch: 8000 | Test loss: 1.13497 | Test acc: 77.23750\n",
            "Test batch: 10000 | Test loss: 1.12505 | Test acc: 77.42000\n",
            "Test loss: 1.12505 | Test acc: 77.42000\n",
            "\n",
            "Epoch: 38\n",
            "Train batch: 100 | Train loss: 0.00033 | Train acc: 100.00000\n",
            "Train batch: 200 | Train loss: 0.00035 | Train acc: 100.00000\n",
            "Train batch: 300 | Train loss: 0.00036 | Train acc: 100.00000\n",
            "Train batch: 400 | Train loss: 0.00037 | Train acc: 100.00000\n",
            "Train batch: 500 | Train loss: 0.00038 | Train acc: 100.00000\n",
            "Train batch: 600 | Train loss: 0.00038 | Train acc: 100.00000\n",
            "Train batch: 700 | Train loss: 0.00039 | Train acc: 100.00000\n",
            "Train loss: 0.00039 | Train acc: 100.00000\n",
            "Test batch: 2000 | Test loss: 1.10421 | Test acc: 77.65000\n",
            "Test batch: 4000 | Test loss: 1.11045 | Test acc: 77.62500\n",
            "Test batch: 6000 | Test loss: 1.11178 | Test acc: 77.50000\n",
            "Test batch: 8000 | Test loss: 1.13025 | Test acc: 77.35000\n",
            "Test batch: 10000 | Test loss: 1.12019 | Test acc: 77.48000\n",
            "Test loss: 1.12019 | Test acc: 77.48000\n",
            "\n",
            "Epoch: 39\n",
            "Train batch: 100 | Train loss: 0.00036 | Train acc: 100.00000\n",
            "Train batch: 200 | Train loss: 0.00037 | Train acc: 100.00000\n",
            "Train batch: 300 | Train loss: 0.00038 | Train acc: 100.00000\n",
            "Train batch: 400 | Train loss: 0.00038 | Train acc: 100.00000\n",
            "Train batch: 500 | Train loss: 0.00039 | Train acc: 100.00000\n",
            "Train batch: 600 | Train loss: 0.00040 | Train acc: 100.00000\n",
            "Train batch: 700 | Train loss: 0.00040 | Train acc: 100.00000\n",
            "Train loss: 0.00040 | Train acc: 100.00000\n",
            "Test batch: 2000 | Test loss: 1.10389 | Test acc: 78.00000\n",
            "Test batch: 4000 | Test loss: 1.10883 | Test acc: 77.67500\n",
            "Test batch: 6000 | Test loss: 1.11027 | Test acc: 77.48333\n",
            "Test batch: 8000 | Test loss: 1.12867 | Test acc: 77.30000\n",
            "Test batch: 10000 | Test loss: 1.11820 | Test acc: 77.47000\n",
            "Test loss: 1.11820 | Test acc: 77.47000\n",
            "\n",
            "Epoch: 40\n",
            "Train batch: 100 | Train loss: 0.00038 | Train acc: 100.00000\n",
            "Train batch: 200 | Train loss: 0.00039 | Train acc: 100.00000\n",
            "Train batch: 300 | Train loss: 0.00040 | Train acc: 100.00000\n",
            "Train batch: 400 | Train loss: 0.00040 | Train acc: 100.00000\n",
            "Train batch: 500 | Train loss: 0.00040 | Train acc: 100.00000\n",
            "Train batch: 600 | Train loss: 0.00041 | Train acc: 100.00000\n",
            "Train batch: 700 | Train loss: 0.00041 | Train acc: 100.00000\n",
            "Train loss: 0.00042 | Train acc: 100.00000\n",
            "Test batch: 2000 | Test loss: 1.10640 | Test acc: 78.00000\n",
            "Test batch: 4000 | Test loss: 1.10954 | Test acc: 77.75000\n",
            "Test batch: 6000 | Test loss: 1.11129 | Test acc: 77.58333\n",
            "Test batch: 8000 | Test loss: 1.12924 | Test acc: 77.41250\n",
            "Test batch: 10000 | Test loss: 1.11776 | Test acc: 77.59000\n",
            "Test loss: 1.11776 | Test acc: 77.59000\n",
            "\n",
            "Epoch: 41\n",
            "Train batch: 100 | Train loss: 0.00039 | Train acc: 100.00000\n",
            "Train batch: 200 | Train loss: 0.00040 | Train acc: 100.00000\n",
            "Train batch: 300 | Train loss: 0.00041 | Train acc: 100.00000\n",
            "Train batch: 400 | Train loss: 0.00041 | Train acc: 100.00000\n",
            "Train batch: 500 | Train loss: 0.00042 | Train acc: 100.00000\n",
            "Train batch: 600 | Train loss: 0.00042 | Train acc: 100.00000\n",
            "Train batch: 700 | Train loss: 0.00043 | Train acc: 100.00000\n",
            "Train loss: 0.00043 | Train acc: 100.00000\n",
            "Test batch: 2000 | Test loss: 1.10491 | Test acc: 78.00000\n",
            "Test batch: 4000 | Test loss: 1.10715 | Test acc: 77.75000\n",
            "Test batch: 6000 | Test loss: 1.10924 | Test acc: 77.58333\n",
            "Test batch: 8000 | Test loss: 1.12716 | Test acc: 77.33750\n",
            "Test batch: 10000 | Test loss: 1.11533 | Test acc: 77.56000\n",
            "Test loss: 1.11533 | Test acc: 77.56000\n",
            "\n",
            "Epoch: 42\n",
            "Train batch: 100 | Train loss: 0.00042 | Train acc: 100.00000\n",
            "Train batch: 200 | Train loss: 0.00042 | Train acc: 100.00000\n",
            "Train batch: 300 | Train loss: 0.00042 | Train acc: 100.00000\n",
            "Train batch: 400 | Train loss: 0.00042 | Train acc: 100.00000\n",
            "Train batch: 500 | Train loss: 0.00043 | Train acc: 100.00000\n",
            "Train batch: 600 | Train loss: 0.00043 | Train acc: 100.00000\n",
            "Train batch: 700 | Train loss: 0.00044 | Train acc: 100.00000\n",
            "Train loss: 0.00045 | Train acc: 100.00000\n",
            "Test batch: 2000 | Test loss: 1.10688 | Test acc: 77.90000\n",
            "Test batch: 4000 | Test loss: 1.10863 | Test acc: 77.75000\n",
            "Test batch: 6000 | Test loss: 1.10984 | Test acc: 77.53333\n",
            "Test batch: 8000 | Test loss: 1.12734 | Test acc: 77.28750\n",
            "Test batch: 10000 | Test loss: 1.11458 | Test acc: 77.56000\n",
            "Test loss: 1.11458 | Test acc: 77.56000\n",
            "\n",
            "Epoch: 43\n",
            "Train batch: 100 | Train loss: 0.00042 | Train acc: 100.00000\n",
            "Train batch: 200 | Train loss: 0.00043 | Train acc: 100.00000\n",
            "Train batch: 300 | Train loss: 0.00043 | Train acc: 100.00000\n",
            "Train batch: 400 | Train loss: 0.00045 | Train acc: 100.00000\n",
            "Train batch: 500 | Train loss: 0.00045 | Train acc: 100.00000\n",
            "Train batch: 600 | Train loss: 0.00045 | Train acc: 100.00000\n",
            "Train batch: 700 | Train loss: 0.00045 | Train acc: 100.00000\n",
            "Train loss: 0.00046 | Train acc: 100.00000\n",
            "Test batch: 2000 | Test loss: 1.11140 | Test acc: 78.05000\n",
            "Test batch: 4000 | Test loss: 1.11053 | Test acc: 77.90000\n",
            "Test batch: 6000 | Test loss: 1.11193 | Test acc: 77.53333\n",
            "Test batch: 8000 | Test loss: 1.12846 | Test acc: 77.35000\n",
            "Test batch: 10000 | Test loss: 1.11556 | Test acc: 77.59000\n",
            "Test loss: 1.11556 | Test acc: 77.59000\n",
            "\n",
            "Epoch: 44\n",
            "Train batch: 100 | Train loss: 0.00046 | Train acc: 100.00000\n",
            "Train batch: 200 | Train loss: 0.00046 | Train acc: 100.00000\n",
            "Train batch: 300 | Train loss: 0.00046 | Train acc: 100.00000\n",
            "Train batch: 400 | Train loss: 0.00046 | Train acc: 100.00000\n",
            "Train batch: 500 | Train loss: 0.00046 | Train acc: 100.00000\n",
            "Train batch: 600 | Train loss: 0.00046 | Train acc: 100.00000\n",
            "Train batch: 700 | Train loss: 0.00047 | Train acc: 100.00000\n",
            "Train loss: 0.00047 | Train acc: 100.00000\n",
            "Test batch: 2000 | Test loss: 1.11052 | Test acc: 77.85000\n",
            "Test batch: 4000 | Test loss: 1.10959 | Test acc: 77.80000\n",
            "Test batch: 6000 | Test loss: 1.11181 | Test acc: 77.58333\n",
            "Test batch: 8000 | Test loss: 1.12908 | Test acc: 77.37500\n",
            "Test batch: 10000 | Test loss: 1.11569 | Test acc: 77.65000\n",
            "Test loss: 1.11569 | Test acc: 77.65000\n",
            "\n",
            "Epoch: 45\n",
            "Train batch: 100 | Train loss: 0.00043 | Train acc: 100.00000\n",
            "Train batch: 200 | Train loss: 0.00046 | Train acc: 100.00000\n",
            "Train batch: 300 | Train loss: 0.00047 | Train acc: 100.00000\n",
            "Train batch: 400 | Train loss: 0.00046 | Train acc: 100.00000\n",
            "Train batch: 500 | Train loss: 0.00047 | Train acc: 100.00000\n",
            "Train batch: 600 | Train loss: 0.00047 | Train acc: 100.00000\n",
            "Train batch: 700 | Train loss: 0.00047 | Train acc: 100.00000\n",
            "Train loss: 0.00048 | Train acc: 100.00000\n",
            "Test batch: 2000 | Test loss: 1.11453 | Test acc: 77.80000\n",
            "Test batch: 4000 | Test loss: 1.11369 | Test acc: 77.85000\n",
            "Test batch: 6000 | Test loss: 1.11554 | Test acc: 77.61667\n",
            "Test batch: 8000 | Test loss: 1.13214 | Test acc: 77.33750\n",
            "Test batch: 10000 | Test loss: 1.11778 | Test acc: 77.60000\n",
            "Test loss: 1.11778 | Test acc: 77.60000\n",
            "\n",
            "Epoch: 46\n",
            "Train batch: 100 | Train loss: 0.00044 | Train acc: 100.00000\n",
            "Train batch: 200 | Train loss: 0.00047 | Train acc: 100.00000\n",
            "Train batch: 300 | Train loss: 0.00047 | Train acc: 100.00000\n",
            "Train batch: 400 | Train loss: 0.00048 | Train acc: 100.00000\n",
            "Train batch: 500 | Train loss: 0.00048 | Train acc: 100.00000\n",
            "Train batch: 600 | Train loss: 0.00049 | Train acc: 100.00000\n",
            "Train batch: 700 | Train loss: 0.00049 | Train acc: 100.00000\n",
            "Train loss: 0.00049 | Train acc: 100.00000\n",
            "Test batch: 2000 | Test loss: 1.11940 | Test acc: 77.60000\n",
            "Test batch: 4000 | Test loss: 1.11679 | Test acc: 77.77500\n",
            "Test batch: 6000 | Test loss: 1.11889 | Test acc: 77.55000\n",
            "Test batch: 8000 | Test loss: 1.13526 | Test acc: 77.31250\n",
            "Test batch: 10000 | Test loss: 1.12090 | Test acc: 77.62000\n",
            "Test loss: 1.12090 | Test acc: 77.62000\n",
            "\n",
            "Epoch: 47\n",
            "Train batch: 100 | Train loss: 0.00048 | Train acc: 100.00000\n",
            "Train batch: 200 | Train loss: 0.00049 | Train acc: 100.00000\n",
            "Train batch: 300 | Train loss: 0.00048 | Train acc: 100.00000\n",
            "Train batch: 400 | Train loss: 0.00049 | Train acc: 100.00000\n",
            "Train batch: 500 | Train loss: 0.00049 | Train acc: 100.00000\n",
            "Train batch: 600 | Train loss: 0.00049 | Train acc: 100.00000\n",
            "Train batch: 700 | Train loss: 0.00050 | Train acc: 100.00000\n",
            "Train loss: 0.00050 | Train acc: 100.00000\n",
            "Test batch: 2000 | Test loss: 1.12394 | Test acc: 77.70000\n",
            "Test batch: 4000 | Test loss: 1.11865 | Test acc: 77.80000\n",
            "Test batch: 6000 | Test loss: 1.12131 | Test acc: 77.56667\n",
            "Test batch: 8000 | Test loss: 1.13739 | Test acc: 77.31250\n",
            "Test batch: 10000 | Test loss: 1.12234 | Test acc: 77.64000\n",
            "Test loss: 1.12234 | Test acc: 77.64000\n",
            "\n",
            "Epoch: 48\n",
            "Train batch: 100 | Train loss: 0.00048 | Train acc: 100.00000\n",
            "Train batch: 200 | Train loss: 0.00048 | Train acc: 100.00000\n",
            "Train batch: 300 | Train loss: 0.00049 | Train acc: 100.00000\n",
            "Train batch: 400 | Train loss: 0.00049 | Train acc: 100.00000\n",
            "Train batch: 500 | Train loss: 0.00050 | Train acc: 100.00000\n",
            "Train batch: 600 | Train loss: 0.00050 | Train acc: 100.00000\n",
            "Train batch: 700 | Train loss: 0.00051 | Train acc: 100.00000\n",
            "Train loss: 0.00051 | Train acc: 100.00000\n",
            "Test batch: 2000 | Test loss: 1.12723 | Test acc: 77.20000\n",
            "Test batch: 4000 | Test loss: 1.12229 | Test acc: 77.50000\n",
            "Test batch: 6000 | Test loss: 1.12619 | Test acc: 77.28333\n",
            "Test batch: 8000 | Test loss: 1.14154 | Test acc: 77.06250\n",
            "Test batch: 10000 | Test loss: 1.12589 | Test acc: 77.39000\n",
            "Test loss: 1.12589 | Test acc: 77.39000\n",
            "\n",
            "Epoch: 49\n",
            "Train batch: 100 | Train loss: 0.00047 | Train acc: 100.00000\n",
            "Train batch: 200 | Train loss: 0.00049 | Train acc: 100.00000\n",
            "Train batch: 300 | Train loss: 0.00049 | Train acc: 100.00000\n",
            "Train batch: 400 | Train loss: 0.00050 | Train acc: 100.00000\n",
            "Train batch: 500 | Train loss: 0.00051 | Train acc: 100.00000\n",
            "Train batch: 600 | Train loss: 0.00051 | Train acc: 100.00000\n",
            "Train batch: 700 | Train loss: 0.00052 | Train acc: 100.00000\n",
            "Train loss: 0.00052 | Train acc: 100.00000\n",
            "Test batch: 2000 | Test loss: 1.13201 | Test acc: 77.30000\n",
            "Test batch: 4000 | Test loss: 1.12517 | Test acc: 77.60000\n",
            "Test batch: 6000 | Test loss: 1.12984 | Test acc: 77.30000\n",
            "Test batch: 8000 | Test loss: 1.14493 | Test acc: 77.12500\n",
            "Test batch: 10000 | Test loss: 1.12954 | Test acc: 77.42000\n",
            "Test loss: 1.12954 | Test acc: 77.42000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "epochs = 50\n",
        "\n",
        "train_loss, train_acc = [], []\n",
        "test_loss, test_acc = [], []\n",
        "\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    print(f\"Epoch: {epoch}\")\n",
        "\n",
        "    train_loss, train_acc = train_step(model=network,\n",
        "              data_loader=train_loader,\n",
        "              loss_fn=loss_fn,\n",
        "              optimizer=optimizer,\n",
        "              accuracy_fn=accuracy_fn,\n",
        "              device=device)\n",
        "\n",
        "    test_loss, test_acc = test_step(model=network,\n",
        "             data_loader=test_loader,\n",
        "             loss_fn=loss_fn,\n",
        "             accuracy_fn=accuracy_fn,\n",
        "             device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOty1-EvEcm5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c68df830d8434d1f96c67898d94dd176": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b7c3d4c4b8ec4b22a02716889d0d6e23",
              "IPY_MODEL_183f2788fb3f4ae695477a5f74a329bc",
              "IPY_MODEL_51e5ee4bb52d402d98ab494406503a62"
            ],
            "layout": "IPY_MODEL_7e1f8c74c4b94008bf7f5871a2bcd964"
          }
        },
        "b7c3d4c4b8ec4b22a02716889d0d6e23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_252b5a5fd587425c9a81c4c6a22a6c7e",
            "placeholder": "​",
            "style": "IPY_MODEL_cd7b9886353c44edb13aab870135d157",
            "value": "100%"
          }
        },
        "183f2788fb3f4ae695477a5f74a329bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_316677dd0f234414be35a7be798dd544",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_24c988b5935541f8b440a3e88991dcd8",
            "value": 50
          }
        },
        "51e5ee4bb52d402d98ab494406503a62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f312247c77f40a3adc6e7458a6af64b",
            "placeholder": "​",
            "style": "IPY_MODEL_644e11f22b9747afb8f49424f603fd3c",
            "value": " 50/50 [37:21&lt;00:00, 43.94s/it]"
          }
        },
        "7e1f8c74c4b94008bf7f5871a2bcd964": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "252b5a5fd587425c9a81c4c6a22a6c7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd7b9886353c44edb13aab870135d157": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "316677dd0f234414be35a7be798dd544": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24c988b5935541f8b440a3e88991dcd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2f312247c77f40a3adc6e7458a6af64b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "644e11f22b9747afb8f49424f603fd3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}